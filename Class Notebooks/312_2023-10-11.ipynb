{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/briggsc1-erau.edu/Downloads/housing.csv')\n",
    "features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income']\n",
    "target = ['median_house_value']\n",
    "df = df.dropna(subset = features+target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_te,y_tr,y_te = train_test_split(df[features],df[target],\n",
    "                                       test_size = 0.4, random_state = 0)\n",
    "x_va,x_te,y_va,y_te = train_test_split(x_te,y_te,\n",
    "                                       test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "map takes a function and an iterable, applies the function to each thing in the iterable, and returns an iterator containing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(n):\n",
    "    return(2*n)\n",
    "nums = [1,2,3,4]\n",
    "v = map(double, nums)\n",
    "for x in v: print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functools.reduce\n",
    "\n",
    "functools.reduce takes a function (of two arguments) and an iterable, applies the function successively along the iterable, and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def subtract(a,b):\n",
    "    print(f'{a} minus {b} is {a-b}')\n",
    "    return(a-b)\n",
    "reduce(subtract,map(double,nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: once you use an iterator once, it's used up.\n",
    "\n",
    "Q: why does it not work to call reduce(sum,map(double,nums))?\n",
    "\n",
    "You try: use map and reduce to get the sum of the squares up to n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linux terminal: lscpu to see how many CPUs you have.\n",
    "\n",
    "When multiprocessing, you should use at least CPUs - 1 processes. If you have 10 CPUs, use at least 9 processes. Reason: python is single-threaded. Terminal: top to see a process's id (PID), and cat /proc\\/<proc id\\>\\/status to see the number of threads taken by a process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy example for multiprocessing: adding up the squares from 1 to N\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "N=100\n",
    "def test(x):\n",
    "    return(x**2)\n",
    "inpt = range(1,N+1)\n",
    "with Pool(processes = 19) as p:\n",
    "    res = p.map(test,inpt)\n",
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we create a pool of processes, then distribute (map) the function calls over the iterable input.\n",
    "\n",
    "Here's an example where the function has two inputs. It adds up $a^b$ where $a$ ranges from 1 to 3 and $b$ ranges from 0 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    return(x[0]**x[1])\n",
    "inpt = product(range(1,4),range(4))\n",
    "with Pool(processes = 10) as p:\n",
    "    res = p.map(test,inpt)\n",
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this to the problem we had last time with the absolute_error trees taking a long time to train. It will still take a few minutes (depending on how good these lab machines are), but it will be doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = DecisionTreeRegressor(random_state = 0)\n",
    "tr.fit(x_tr,y_tr)\n",
    "ud = tr.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = []\n",
    "def fit_tree(inpt):\n",
    "    max_depth,min_samples_leaf = inpt\n",
    "    tr = DecisionTreeRegressor(max_depth = max_depth,\n",
    "                               min_samples_leaf = min_samples_leaf,\n",
    "                               random_state = 0,\n",
    "                               criterion = 'absolute_error')\n",
    "    tr.fit(x_tr,y_tr)\n",
    "    return(tr)\n",
    "inpt = product(range(1,ud+1),range(1,13))\n",
    "with Pool(processes = 19) as p: # adjust processes according to your hardware\n",
    "    trees = p.map(fit_tree,inpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get trees for the other three criteria (we did this last time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tree(inpt):\n",
    "    max_depth,min_samples_leaf,crit = inpt\n",
    "    tr = DecisionTreeRegressor(max_depth = max_depth,\n",
    "                               min_samples_leaf = min_samples_leaf,\n",
    "                               random_state = 0,\n",
    "                               criterion = crit)\n",
    "    tr.fit(x_tr,y_tr)\n",
    "    return(tr)\n",
    "inpt = product(range(1,ud+1),range(1,13),['squared_error','friedman_mse','poisson'])\n",
    "with Pool(processes = 19) as p:\n",
    "    trees2 = p.map(fit_tree,inpt)\n",
    "trees = trees+trees2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make the dataframe of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for tree in trees:\n",
    "    tr_sc = tree.score(x_tr,y_tr)\n",
    "    va_sc = tree.score(x_va,y_va)\n",
    "    data.append({'crit':tree.criterion,'set':'train','score':tr_sc,'num_leaves':tree.get_n_leaves(),\n",
    "                 'depth':tree.max_depth,'min_samples':tree.min_samples_leaf})\n",
    "    data.append({'crit':tree.criterion,'set':'val','score':va_sc,'num_leaves':tree.get_n_leaves(),\n",
    "                 'depth':tree.max_depth,'min_samples':tree.min_samples_leaf})\n",
    "df_acc = pd.DataFrame(data)\n",
    "# create a multi-indexed dataframe\n",
    "multi = df_acc.set_index(['set','crit','depth','min_samples'])\n",
    "multi = multi.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be difficult to usefully plot all four criteria at once. However, we can get the information we need to comparing the criteria pairwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit1 = 'squared_error' # the bigger dots\n",
    "crit2 = 'absolute_error' # the smaller dots\n",
    "fig,axes = plt.subplots(4,3,figsize = (15,9))\n",
    "fig.tight_layout(pad=5.0)\n",
    "for j in range(12):\n",
    "    axes[j%4,j//4].scatter(range(depth_lower,ud+1),multi.loc[('train',crit1,slice(None),j+1)].score,\n",
    "            color = '#FF0000')\n",
    "    axes[j%4,j//4].scatter(range(depth_lower,ud+1),multi.loc[('val',crit1,slice(None),j+1)].score,\n",
    "            color = '#00FF00')\n",
    "    axes[j%4,j//4].scatter(range(depth_lower,ud+1),multi.loc[('train',crit2,slice(None),j+1)].score,\n",
    "            color = '#770000',s = 10)\n",
    "    axes[j%4,j//4].scatter(range(depth_lower,ud+1),multi.loc[('val',crit2,slice(None),j+1)].score,\n",
    "            color = '#000077',s = 10)\n",
    "    axes[j%4,j//4].set_title(f'min_samples_leaf = {j+1}')\n",
    "    axes[j%4,j//4].grid()\n",
    "    axes[j%4,j//4].set_xticks(range(0,ud+1,2))\n",
    "    axes[j%4,j//4].set_ylim(0,1.1)\n",
    "    axes[j%4,j//4].set_yticks(np.linspace(0,1,6))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not so easy to eyeball, so we can subset the scores dataframe with set == val, then sort by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc[(df_acc['set']=='val')&(df_acc['crit']!='absolute_error')].sort_values('score',ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "absolute_error appears to dominate. But the others aren't **that** far behind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best, by criterion')\n",
    "print(f'    score        criterion          max_depth        min_samples_leaf')\n",
    "print('  '+'-'*63)\n",
    "for crit in df_acc['crit'].unique():\n",
    "    idx = df_acc[(df_acc['crit']==crit)&(df_acc['set']=='val')]['score'].idxmax()\n",
    "    row = df_acc.loc[idx]\n",
    "    \n",
    "    print(f'  | {round(best,5):<10} | {row.crit:<16} | {row.depth:<14} | {row.min_samples:>2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "Now we discuss bootstrapping. Bootstrapping is a fundamental statistical technique, especially in nonparametric statistics. We use repeated sampling to empirically formulate estimates of population parameters without supposing a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('https://gist.github.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = [] # to hold the y-intercepts of the lines of best fit\n",
    "coefs = [] # to hold the slopes\n",
    "# 100 iterations: sample the data, fit a line, and get the parameters\n",
    "for i in range(100):\n",
    "    choices = np.random.choice(range(150), size=50, replace=True)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(iris[['petal.width']].loc[choices],iris[['petal.length']].loc[choices])\n",
    "    intercepts.append(lr.intercept_.item())\n",
    "    coefs.append(lr.coef_.item())\n",
    "# collect the parameter samples into a dataframe\n",
    "reg_df = pd.DataFrame({'coef':coefs,'intercept':intercepts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which will take a set of x-values and a dataframe with coef and intercept\n",
    "# it will return the min and max y-values of all the lines for each x-value\n",
    "def minmax_preds(xs,df):\n",
    "    mins = []\n",
    "    maxes = []\n",
    "    for x in xs:\n",
    "        x_max = df.iloc[0]['coef']*x+df.iloc[0]['intercept']\n",
    "        x_min = df.iloc[0]['coef']*x+df.iloc[0]['intercept']\n",
    "        for i,row in df.iterrows():\n",
    "            y = row['coef']*x+row['intercept']\n",
    "            if y<x_min:\n",
    "                x_min = y\n",
    "            if y>x_max:\n",
    "                x_max = y\n",
    "        mins.append(x_min)\n",
    "        maxes.append(x_max)\n",
    "    return(mins,maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the xs to be the range of petal.width values\n",
    "xs = np.linspace(0,max(iris['petal.width']),50)\n",
    "# get the min and max prediction of all 100 lines for each x-value\n",
    "ymins,ymaxes = minmax_preds(xs,reg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lower and upper bounds of the line predictions for each x-value\n",
    "fig,ax = plt.subplots(figsize = (6,4))\n",
    "ax.scatter(xs,ymins,s=3,color = '#FF0000')\n",
    "ax.scatter(xs,ymaxes,s=3,color = '#FF0000')\n",
    "ax.scatter(iris['petal.width'],iris['petal.length'])\n",
    "ax.set_title('Iris / lines of best fit 95% CI')\n",
    "ax.set_xlabel('petal width')\n",
    "ax.set_ylabel('petal length');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "k-fold cross-validation is a sort of bootstrapping. It means after splitting off the test data from a dataset, breaking the remainder into k evenly-sized pieces. Each of the k pieces is, in turn, treated as a validation set, while the other k-1 pieces act as training sets. Doing so will give us a collection of parameters which can be averaged. This approach is less prone to overfit since it uses several training sets. Let's use k-fold cross validation on the iris set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_te,y_tr,y_te = train_test_split(iris[['petal.width']],iris[['petal.length']],\n",
    "                                      test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3) # by default, the input will not be shuffled!\n",
    "kf.get_n_splits(x_tr) # define splits on the x_tr indices\n",
    "# kf.split(x_tr) returns a generator. let's investigate:\n",
    "for i, (train_index, val_index) in enumerate(kf.split(x_tr)):\n",
    "    print(f'FOLD {i}:')\n",
    "    print(f'  TRAIN INDEX={train_index}')\n",
    "    print(f'  VAL INDEX={val_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "intercepts = []\n",
    "for fold,(train_index,val_index) in enumerate(kf.split(x_tr)):\n",
    "    lr.fit(x_tr.iloc[train_index],y_tr.iloc[train_index])\n",
    "    coefs.append(lr.coef_)\n",
    "    intercepts.append(lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = round(np.mean(coefs),3)\n",
    "intercept = round(np.mean(intercepts),3)\n",
    "print(f'coef: {coef}\\nintercept: {intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the k-folds average line of best fit vs the 95% CI min/max\n",
    "fig,ax = plt.subplots(figsize = (6,4))\n",
    "ax.scatter(xs,ymins,s=3,color = '#FF0000',label = '95% CI min/max')\n",
    "ax.scatter(xs,ymaxes,s=3,color = '#FF0000')\n",
    "ax.scatter(iris['petal.width'],iris['petal.length'])\n",
    "ax.plot(xs,[x*coef+intercept for x in xs],color = '#000000',label = 'k-fold avg')\n",
    "ax.set_title('Iris / lines of best fit 95% CI')\n",
    "ax.set_xlabel('petal width')\n",
    "ax.set_ylabel('petal length')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with more folds!\n",
    "folds = 10\n",
    "kf = KFold(n_splits=folds)\n",
    "kf.get_n_splits(x_tr)\n",
    "coefs = []\n",
    "intercepts = []\n",
    "for fold,(train_index,val_index) in enumerate(kf.split(x_tr)):\n",
    "    lr.fit(x_tr.iloc[train_index],y_tr.iloc[train_index])\n",
    "    coefs.append(lr.coef_)\n",
    "    intercepts.append(lr.intercept_)\n",
    "fig,ax = plt.subplots(figsize = (6,4))\n",
    "ax.scatter(xs,ymins,s=3,color = '#FF0000',label = '95% CI min/max')\n",
    "ax.scatter(xs,ymaxes,s=3,color = '#FF0000')\n",
    "ax.scatter(iris['petal.width'],iris['petal.length'])\n",
    "ax.plot(xs,[x*coef+intercept for x in xs],color = '#000000',label = f'{folds}-fold avg')\n",
    "ax.set_title('Iris / lines of best fit 95% CI')\n",
    "ax.set_xlabel('petal width')\n",
    "ax.set_ylabel('petal length')\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
