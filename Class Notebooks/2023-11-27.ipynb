{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'keras.src' has no attribute 'utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mnist\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BytesIO \u001B[38;5;28;01mas\u001B[39;00m sio\n",
      "File \u001B[1;32mC:\\repos\\DS312\\venv\\Lib\\site-packages\\keras\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n",
      "File \u001B[1;32mC:\\repos\\DS312\\venv\\Lib\\site-packages\\keras\\__internal__\\__init__.py:6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m losses\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizers\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n",
      "File \u001B[1;32mC:\\repos\\DS312\\venv\\Lib\\site-packages\\keras\\__internal__\\models\\__init__.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcloning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clone_and_build_model\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcloning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m in_place_subclassed_model_state_restoration\n",
      "File \u001B[1;32mC:\\repos\\DS312\\venv\\Lib\\site-packages\\keras\\src\\__init__.py:21\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \n\u001B[0;32m     17\u001B[0m \u001B[38;5;124;03mDetailed documentation and user guides are available at\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03m[keras.io](https://keras.io).\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n",
      "File \u001B[1;32mC:\\repos\\DS312\\venv\\Lib\\site-packages\\keras\\src\\applications\\__init__.py:41\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EfficientNetV2M\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mefficientnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EfficientNetV2S\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_resnet_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InceptionResNetV2\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minception_v3\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InceptionV3\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmobilenet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MobileNet\n",
      "File \u001B[1;32mC:\\repos\\DS312\\venv\\Lib\\site-packages\\keras\\src\\applications\\inception_resnet_v2.py:324\u001B[0m\n\u001B[0;32m    320\u001B[0m         x \u001B[38;5;241m=\u001B[39m layers\u001B[38;5;241m.\u001B[39mActivation(activation, name\u001B[38;5;241m=\u001B[39mac_name)(x)\n\u001B[0;32m    321\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;129m@keras\u001B[39m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241m.\u001B[39mregister_keras_serializable()\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mCustomScaleLayer\u001B[39;00m(keras_layers\u001B[38;5;241m.\u001B[39mLayer):\n\u001B[0;32m    326\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, scale, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'keras.src' has no attribute 'utils' (most likely due to a circular import)"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 13:33:49.635065: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-02 13:33:49.635121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-02 13:33:49.636846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-02 13:33:49.643962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 13:33:50.439931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image\n",
    "from io import BytesIO as sio\n",
    "import IPython.display as ipd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# load the data\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m (x_tr, y_tr), (x_te, y_te) \u001B[38;5;241m=\u001B[39m \u001B[43mmnist\u001B[49m\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# the inputs are given in 2d arrays, and we need them to be 1d arrays\u001B[39;00m\n\u001B[0;32m      5\u001B[0m x_tr_2d \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m x_tr]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'mnist' is not defined"
     ]
    }
   ],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
   "source": [
    "# load the data\n",
    "(x_tr, y_tr), (x_te, y_te) = mnist.load_data()\n",
    "\n",
    "# the inputs are given in 2d arrays, and we need them to be 1d arrays\n",
    "x_tr_2d = [x.reshape(-1,) for x in x_tr]\n",
    "x_te_2d = [x.reshape(-1,) for x in x_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "activation = ['logistic', 'tanh', 'relu']\n",
    "learning_rate = ['constant', 'adaptive']\n",
    "learning_rate_init = np.linspace(0.0005,0.002,4)\n",
    "param_grid = dict(activation = activation,\n",
    "                  learning_rate = learning_rate,\n",
    "                  learning_rate_init = learning_rate_init)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 4,
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model and define the grid to search\n",
    "nn = MLPClassifier()\n",
    "grid = GridSearchCV(estimator=nn, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the grid (train models)\n",
    "grid_result = grid.fit(x_tr_2d, y_tr)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scoring': None,\n",
       " 'estimator': MLPClassifier(),\n",
       " 'n_jobs': -1,\n",
       " 'refit': True,\n",
       " 'cv': 3,\n",
       " 'verbose': 0,\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'error_score': nan,\n",
       " 'return_train_score': False,\n",
       " 'param_grid': {'activation': ['logistic', 'tanh', 'relu'],\n",
       "  'learning_rate': ['constant', 'adaptive'],\n",
       "  'learning_rate_init': array([0.0005, 0.001 , 0.0015, 0.002 ])},\n",
       " 'multimetric_': False,\n",
       " 'best_index_': 0,\n",
       " 'best_score_': 0.9600999999999998,\n",
       " 'best_params_': {'activation': 'logistic',\n",
       "  'learning_rate': 'constant',\n",
       "  'learning_rate_init': 0.0005},\n",
       " 'best_estimator_': MLPClassifier(activation='logistic', learning_rate_init=0.0005),\n",
       " 'refit_time_': 113.855144739151,\n",
       " 'scorer_': <function sklearn.metrics._scorer._passthrough_scorer(estimator, *args, **kwargs)>,\n",
       " 'cv_results_': {'mean_fit_time': array([568.52149256, 274.35483925, 291.97218657, 193.56794063,\n",
       "         481.94801148, 411.88734667, 381.73154418, 300.25192356,\n",
       "         404.36706217, 313.13908394, 264.94753361, 303.04226224,\n",
       "         623.55226556, 372.53001213, 400.42738565, 384.04465294,\n",
       "         273.14909561, 304.95270626, 226.60083524, 252.68672784,\n",
       "         228.24274429, 242.83898481, 191.68539405, 131.65336839]),\n",
       "  'std_fit_time': array([ 89.04210839,  19.84223886, 105.33059236,  53.42036189,\n",
       "         131.08088928, 155.04616701,  35.54175058,  90.02494035,\n",
       "          40.930178  ,  50.58573851,  29.53912366, 117.46234712,\n",
       "          24.17668383,  70.07003841,  27.29924502, 182.1455871 ,\n",
       "          75.3006973 ,  66.02705677,  67.96936352,  30.17315173,\n",
       "          38.31057605,   6.70191015,  10.44965956,  30.17278613]),\n",
       "  'mean_score_time': array([0.67359789, 0.73863864, 0.59037169, 0.55236673, 0.62993272,\n",
       "         0.61560551, 0.55342658, 0.53609331, 0.58941746, 0.58536275,\n",
       "         0.52012976, 0.5903155 , 0.54022336, 0.60481326, 0.53244861,\n",
       "         0.56338151, 0.6261278 , 0.53790124, 0.57543166, 0.55495   ,\n",
       "         0.38805612, 0.15684946, 0.18785214, 0.17297101]),\n",
       "  'std_score_time': array([0.16038708, 0.15490744, 0.02209836, 0.04125492, 0.05742105,\n",
       "         0.06627988, 0.0028617 , 0.00942053, 0.04449967, 0.08335788,\n",
       "         0.01795143, 0.03551971, 0.00289838, 0.05670421, 0.02761359,\n",
       "         0.04422434, 0.08585258, 0.00442025, 0.07069184, 0.09907721,\n",
       "         0.13878265, 0.03944242, 0.04636536, 0.09472282]),\n",
       "  'param_activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                     'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                     'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                     'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                     'relu'],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_learning_rate': masked_array(data=['constant', 'constant', 'constant', 'constant',\n",
       "                     'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                     'constant', 'constant', 'constant', 'constant',\n",
       "                     'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                     'constant', 'constant', 'constant', 'constant',\n",
       "                     'adaptive', 'adaptive', 'adaptive', 'adaptive'],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_learning_rate_init': masked_array(data=[0.0005, 0.001, 0.0015, 0.002, 0.0005, 0.001, 0.0015,\n",
       "                     0.002, 0.0005, 0.001, 0.0015, 0.002, 0.0005, 0.001,\n",
       "                     0.0015, 0.002, 0.0005, 0.001, 0.0015, 0.002, 0.0005,\n",
       "                     0.001, 0.0015, 0.002],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'activation': 'logistic',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.0005},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.001},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.0015},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.002},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.0005},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.001},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.0015},\n",
       "   {'activation': 'logistic',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.002},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.0005},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.001},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.0015},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.002},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.0005},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.001},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.0015},\n",
       "   {'activation': 'tanh',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.002},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.0005},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.001},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.0015},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'constant',\n",
       "    'learning_rate_init': 0.002},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.0005},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.001},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.0015},\n",
       "   {'activation': 'relu',\n",
       "    'learning_rate': 'adaptive',\n",
       "    'learning_rate_init': 0.002}],\n",
       "  'split0_test_score': array([0.96115, 0.94575, 0.933  , 0.93845, 0.9589 , 0.94655, 0.94695,\n",
       "         0.93365, 0.95535, 0.93445, 0.9276 , 0.9094 , 0.95205, 0.9309 ,\n",
       "         0.93115, 0.9248 , 0.9575 , 0.9605 , 0.9499 , 0.9451 , 0.95915,\n",
       "         0.96225, 0.95325, 0.94365]),\n",
       "  'split1_test_score': array([0.96005, 0.94815, 0.94635, 0.92295, 0.95585, 0.9505 , 0.93765,\n",
       "         0.93985, 0.94805, 0.9412 , 0.92035, 0.9308 , 0.9528 , 0.9402 ,\n",
       "         0.92965, 0.9291 , 0.9581 , 0.9548 , 0.9515 , 0.94155, 0.9611 ,\n",
       "         0.9578 , 0.95455, 0.94925]),\n",
       "  'split2_test_score': array([0.9591 , 0.9455 , 0.9434 , 0.93245, 0.9608 , 0.9559 , 0.9427 ,\n",
       "         0.92675, 0.9486 , 0.93445, 0.93   , 0.915  , 0.9552 , 0.93535,\n",
       "         0.9312 , 0.91165, 0.96435, 0.96165, 0.95785, 0.9462 , 0.95795,\n",
       "         0.95855, 0.9556 , 0.9432 ]),\n",
       "  'mean_test_score': array([0.9601    , 0.94646667, 0.94091667, 0.93128333, 0.95851667,\n",
       "         0.95098333, 0.94243333, 0.93341667, 0.95066667, 0.9367    ,\n",
       "         0.92598333, 0.9184    , 0.95335   , 0.93548333, 0.93066667,\n",
       "         0.92185   , 0.95998333, 0.95898333, 0.95308333, 0.94428333,\n",
       "         0.9594    , 0.95953333, 0.95446667, 0.94536667]),\n",
       "  'std_test_score': array([0.00083766, 0.00119466, 0.00572601, 0.0063814 , 0.00203893,\n",
       "         0.00383239, 0.00380139, 0.0053506 , 0.00331922, 0.00318198,\n",
       "         0.0041021 , 0.00906127, 0.0013435 , 0.00379788, 0.00071918,\n",
       "         0.00742305, 0.0030974 , 0.00299509, 0.00343325, 0.00198424,\n",
       "         0.00129808, 0.00194522, 0.00096119, 0.00275207]),\n",
       "  'rank_test_score': array([ 1, 12, 16, 20,  6, 10, 15, 19, 11, 17, 22, 24,  8, 18, 21, 23,  2,\n",
       "          5,  9, 14,  4,  3,  7, 13], dtype=int32)},\n",
       " 'n_splits_': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
   "source": [
    "grid_result.__dict__"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.960100, using\n",
      "   activation: logistic\n",
      "   learning_rate: constant\n",
      "   learning_rate_init: 0.0005\n",
      "\n",
      "\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
   "source": [
    "print(f'best score: {grid_result.best_score_:f}, using')\n",
    "for key in grid_result.best_params_.keys():\n",
    "    print(f'   {key}: {grid_result.best_params_[key]}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'logistic',\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.0005}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([568.52149256, 274.35483925, 291.97218657, 193.56794063,\n",
       "        481.94801148, 411.88734667, 381.73154418, 300.25192356,\n",
       "        404.36706217, 313.13908394, 264.94753361, 303.04226224,\n",
       "        623.55226556, 372.53001213, 400.42738565, 384.04465294,\n",
       "        273.14909561, 304.95270626, 226.60083524, 252.68672784,\n",
       "        228.24274429, 242.83898481, 191.68539405, 131.65336839]),\n",
       " 'std_fit_time': array([ 89.04210839,  19.84223886, 105.33059236,  53.42036189,\n",
       "        131.08088928, 155.04616701,  35.54175058,  90.02494035,\n",
       "         40.930178  ,  50.58573851,  29.53912366, 117.46234712,\n",
       "         24.17668383,  70.07003841,  27.29924502, 182.1455871 ,\n",
       "         75.3006973 ,  66.02705677,  67.96936352,  30.17315173,\n",
       "         38.31057605,   6.70191015,  10.44965956,  30.17278613]),\n",
       " 'mean_score_time': array([0.67359789, 0.73863864, 0.59037169, 0.55236673, 0.62993272,\n",
       "        0.61560551, 0.55342658, 0.53609331, 0.58941746, 0.58536275,\n",
       "        0.52012976, 0.5903155 , 0.54022336, 0.60481326, 0.53244861,\n",
       "        0.56338151, 0.6261278 , 0.53790124, 0.57543166, 0.55495   ,\n",
       "        0.38805612, 0.15684946, 0.18785214, 0.17297101]),\n",
       " 'std_score_time': array([0.16038708, 0.15490744, 0.02209836, 0.04125492, 0.05742105,\n",
       "        0.06627988, 0.0028617 , 0.00942053, 0.04449967, 0.08335788,\n",
       "        0.01795143, 0.03551971, 0.00289838, 0.05670421, 0.02761359,\n",
       "        0.04422434, 0.08585258, 0.00442025, 0.07069184, 0.09907721,\n",
       "        0.13878265, 0.03944242, 0.04636536, 0.09472282]),\n",
       " 'param_activation': masked_array(data=['logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=['constant', 'constant', 'constant', 'constant',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive',\n",
       "                    'constant', 'constant', 'constant', 'constant',\n",
       "                    'adaptive', 'adaptive', 'adaptive', 'adaptive'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate_init': masked_array(data=[0.0005, 0.001, 0.0015, 0.002, 0.0005, 0.001, 0.0015,\n",
       "                    0.002, 0.0005, 0.001, 0.0015, 0.002, 0.0005, 0.001,\n",
       "                    0.0015, 0.002, 0.0005, 0.001, 0.0015, 0.002, 0.0005,\n",
       "                    0.001, 0.0015, 0.002],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'activation': 'logistic',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.0005},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.001},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.0015},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.002},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.0005},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.001},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.0015},\n",
       "  {'activation': 'logistic',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.002},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.0005},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.001},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.0015},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.002},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.0005},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.001},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.0015},\n",
       "  {'activation': 'tanh',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.002},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.0005},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.001},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.0015},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'constant',\n",
       "   'learning_rate_init': 0.002},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.0005},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.001},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.0015},\n",
       "  {'activation': 'relu',\n",
       "   'learning_rate': 'adaptive',\n",
       "   'learning_rate_init': 0.002}],\n",
       " 'split0_test_score': array([0.96115, 0.94575, 0.933  , 0.93845, 0.9589 , 0.94655, 0.94695,\n",
       "        0.93365, 0.95535, 0.93445, 0.9276 , 0.9094 , 0.95205, 0.9309 ,\n",
       "        0.93115, 0.9248 , 0.9575 , 0.9605 , 0.9499 , 0.9451 , 0.95915,\n",
       "        0.96225, 0.95325, 0.94365]),\n",
       " 'split1_test_score': array([0.96005, 0.94815, 0.94635, 0.92295, 0.95585, 0.9505 , 0.93765,\n",
       "        0.93985, 0.94805, 0.9412 , 0.92035, 0.9308 , 0.9528 , 0.9402 ,\n",
       "        0.92965, 0.9291 , 0.9581 , 0.9548 , 0.9515 , 0.94155, 0.9611 ,\n",
       "        0.9578 , 0.95455, 0.94925]),\n",
       " 'split2_test_score': array([0.9591 , 0.9455 , 0.9434 , 0.93245, 0.9608 , 0.9559 , 0.9427 ,\n",
       "        0.92675, 0.9486 , 0.93445, 0.93   , 0.915  , 0.9552 , 0.93535,\n",
       "        0.9312 , 0.91165, 0.96435, 0.96165, 0.95785, 0.9462 , 0.95795,\n",
       "        0.95855, 0.9556 , 0.9432 ]),\n",
       " 'mean_test_score': array([0.9601    , 0.94646667, 0.94091667, 0.93128333, 0.95851667,\n",
       "        0.95098333, 0.94243333, 0.93341667, 0.95066667, 0.9367    ,\n",
       "        0.92598333, 0.9184    , 0.95335   , 0.93548333, 0.93066667,\n",
       "        0.92185   , 0.95998333, 0.95898333, 0.95308333, 0.94428333,\n",
       "        0.9594    , 0.95953333, 0.95446667, 0.94536667]),\n",
       " 'std_test_score': array([0.00083766, 0.00119466, 0.00572601, 0.0063814 , 0.00203893,\n",
       "        0.00383239, 0.00380139, 0.0053506 , 0.00331922, 0.00318198,\n",
       "        0.0041021 , 0.00906127, 0.0013435 , 0.00379788, 0.00071918,\n",
       "        0.00742305, 0.0030974 , 0.00299509, 0.00343325, 0.00198424,\n",
       "        0.00129808, 0.00194522, 0.00096119, 0.00275207]),\n",
       " 'rank_test_score': array([ 1, 12, 16, 20,  6, 10, 15, 19, 11, 17, 22, 24,  8, 18, 21, 23,  2,\n",
       "         5,  9, 14,  4,  3,  7, 13], dtype=int32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all model configurations and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9601, 0.00084, 'logistic', 'constant', 0.0005),\n",
       " (0.96, 0.0031, 'relu', 'constant', 0.0005),\n",
       " (0.9595, 0.00195, 'relu', 'adaptive', 0.001),\n",
       " (0.9594, 0.0013, 'relu', 'adaptive', 0.0005),\n",
       " (0.959, 0.003, 'relu', 'constant', 0.001),\n",
       " (0.9585, 0.00204, 'logistic', 'adaptive', 0.0005),\n",
       " (0.9545, 0.00096, 'relu', 'adaptive', 0.0015),\n",
       " (0.9534, 0.00134, 'tanh', 'adaptive', 0.0005),\n",
       " (0.9531, 0.00343, 'relu', 'constant', 0.0015),\n",
       " (0.951, 0.00383, 'logistic', 'adaptive', 0.001),\n",
       " (0.9507, 0.00332, 'tanh', 'constant', 0.0005),\n",
       " (0.9465, 0.00119, 'logistic', 'constant', 0.001),\n",
       " (0.9454, 0.00275, 'relu', 'adaptive', 0.002),\n",
       " (0.9443, 0.00198, 'relu', 'constant', 0.002),\n",
       " (0.9424, 0.0038, 'logistic', 'adaptive', 0.0015),\n",
       " (0.9409, 0.00573, 'logistic', 'constant', 0.0015),\n",
       " (0.9367, 0.00318, 'tanh', 'constant', 0.001),\n",
       " (0.9355, 0.0038, 'tanh', 'adaptive', 0.001),\n",
       " (0.9334, 0.00535, 'logistic', 'adaptive', 0.002),\n",
       " (0.9313, 0.00638, 'logistic', 'constant', 0.002),\n",
       " (0.9307, 0.00072, 'tanh', 'adaptive', 0.0015),\n",
       " (0.926, 0.0041, 'tanh', 'constant', 0.0015),\n",
       " (0.9218, 0.00742, 'tanh', 'adaptive', 0.002),\n",
       " (0.9184, 0.00906, 'tanh', 'constant', 0.002)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "res = []\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    res.append((round(mean,4), round(stdev,5), *param.values()))\n",
    "res.sort(reverse=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Let's explore CNN components. First we see how convolution layers work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array to which we'll apply the filter\n",
    "data = np.asarray([0, 0, 0, 1, 1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras expects 1D arrays to come in a 3D package. The first dimension is for the sample (i.e. one sample among many), the second dimension is for the length of each array, and the third dimension is for the channel (as in color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(1, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential is a class from keras that lets us stack together layer one-at-a-time into a single model. Conv1D and Conv2D are convolusional layers for application to sets of 1D and 2D arrays respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use one filter with a length of 3, so we'll add one convolution layer to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(1, 3, input_shape=(10, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vertical line detector. The final weight is a bias., added to all outputs.\n",
    "weights = [np.asarray([[[0]],[[1]],[[0]]]), np.asarray([0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the weights in the model\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter to input data\n",
    "yhat = model.predict(data)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we're really just detecting 1s, and not detecting edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [np.asarray([[[-1]],[[1]],[[0]]]), np.asarray([0])]\n",
    "model.set_weights(weights)\n",
    "yhat = model.predict(data)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply a filter to a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input data\n",
    "data = np.asarray(\n",
    "    [[[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0]],\n",
    "     \n",
    "     [[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0]]])\n",
    "\n",
    "data = data.reshape(2, 8, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vertical line detector\n",
    "detector = [[[[-1]],[[1]],[[0]]],\n",
    "            [[[-1]],[[1]],[[0]]],\n",
    "            [[[-1]],[[1]],[[0]]]]\n",
    "weights = [np.asarray(detector), np.asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter to input data\n",
    "yhat = model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 2x6x6x1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can arrange the output into a 2D grids for nice printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first input grid, vertical edge detector\n",
    "print(yhat[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second input grid (horizontal stripe), vertical edge detector\n",
    "print(yhat[1,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that our filter identifies vertical edges but ignores horizontal edges. We can make a filter to detect horizontal edges and ignore vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's include two filters with this model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(2, (3,3), input_shape=(8, 8, 1)))\n",
    "\n",
    "# define vertical and horizontal line detectors\n",
    "detector = [[[[-1,-1]],[[1,-1]],[[0,-1]]],\n",
    "            [[[-1,1]],[[1,1]],[[0,1]]],\n",
    "            [[[-1,0]],[[1,0]],[[0,0]]]]\n",
    "            \n",
    "# our bias vector must have two components\n",
    "weights = [np.asarray(detector), np.asarray([0.0,0])]\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply filter to input data\n",
    "yhat = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the effects of the filters on the first input array\n",
    "yhat[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the effects of the filters on the second input array\n",
    "yhat[:,:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we discuss [pooling](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/).\n",
    "\n",
    "Pooling is an operation done to a numeric grid (e.g. image), similar to applying a convolution. However, the pooling filter is usually 2x2 and applied with a stride of 2 (no overlap between applications). For each 2x2 subgrid of our image, a scalar is produced. Max pooling takes the max of of the four pixels, while average pooling takes the average of the four pixels. The image is thus down sampled by a factor of 4.\n",
    "\n",
    "The utility of pooling is that sometimes features we're trying to detect may change their location slightly within an image. The down sampling involved in pooling gives us a margin of error for the location of features we're detecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of max pooling\n",
    "from numpy import asarray\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "# define input data\n",
    "data = [[0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0],\n",
    " [0, 0, 0, 1, 1, 1, 0, 0]]\n",
    "data = asarray(data)\n",
    "data = data.reshape(1, 8, 8, 1)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1)))\n",
    "\n",
    "# add a pooling layer\n",
    "model.add(MaxPooling2D())\n",
    "# model.add(AveragePooling2D())\n",
    "\n",
    "# summarize model\n",
    "model.summary()\n",
    "# define a vertical line detector\n",
    "detector = [[[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]],\n",
    "            [[[0]],[[1]],[[0]]]]\n",
    "weights = [asarray(detector), asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model.set_weights(weights)\n",
    "# apply filter to input data\n",
    "yhat = model.predict(data)\n",
    "# enumerate rows\n",
    "for r in range(yhat.shape[1]):\n",
    "    # print each column in the row\n",
    "    print([yhat[0,r,c,0] for c in range(yhat.shape[2])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.10"
=======
   "version": "3.10.12"
>>>>>>> 14ce6cc (notebooks and HW 6 and 7)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
