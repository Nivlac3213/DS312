{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap: last time we discussed random forests. We saw that if we train a bunch of unrestricted trees in the same way, their errors are correlated and a majority vote of a bunch is not much better than one tree by itself.\n",
    "\n",
    "We made the errors uncorrelated by training the trees on different subsets of the feature set, getting accuracy comparable to the sklearn implementation of random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  color  \n",
       "0      9.4        5      1  \n",
       "1      9.8        5      1  \n",
       "2      9.8        5      1  \n",
       "3      9.8        6      1  \n",
       "4      9.4        5      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data comes from https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "df_red = pd.read_csv('data/winequality-red.csv',sep = ';')\n",
    "df_red['color'] = 1\n",
    "df_white = pd.read_csv('data/winequality-white.csv',sep = ';')\n",
    "df_white['color'] = 0\n",
    "df = pd.concat([df_red,df_white])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap plot\n",
    "# fig,ax = plt.subplots(figsize=(10, 7))\n",
    "# ax = sns.heatmap(df.corr(), vmin=-1, vmax=1,cmap='vlag',annot=True)\n",
    "# ax.set_xticklabels(ax.get_xticklabels(),rotation =90)\n",
    "# ax.set_yticklabels(ax.get_yticklabels(),rotation=0)\n",
    "# ax.set_title('wine', pad = 25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll predict target: quality using the other columns as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "       'pH', 'sulphates', 'alcohol', 'color']\n",
    "target = ['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,x_te,y_tr,y_te = train_test_split(df[features],df[target],\n",
    "                                       test_size = 0.2)\n",
    "x_tr,x_va,y_tr,y_va = train_test_split(x_tr,y_tr,\n",
    "                                       test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briggsc1-erau.edu/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quality    0.462691\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping random features\n",
    "num_trees = 100\n",
    "fss = [] # a list to hold the list of feature subsets\n",
    "for i in range(num_trees):\n",
    "    # we need a random binary array with not all 0s\n",
    "    array = np.random.randint(0,2,12)\n",
    "    while sum(array)==0:\n",
    "        array = np.random.randint(0,2,12)\n",
    "    fs = np.array(features)[[bool(x) for x in np.random.randint(0,2,12)]]\n",
    "    fss.append(df[fs].columns)\n",
    "\n",
    "trees = []\n",
    "for i in range(num_trees):\n",
    "    tr = DecisionTreeRegressor(random_state = np.random.randint(10**7))\n",
    "    x_tr_subset = x_tr[fss[i]]\n",
    "    tr.fit(x_tr_subset,y_tr)\n",
    "    trees.append(tr)\n",
    "\n",
    "def forest_pred(forest,inputs,fss):\n",
    "    preds = []\n",
    "    for tree,fs in zip(forest,fss):\n",
    "        preds.append(tree.predict(inputs[fs]))\n",
    "    avgs = sum(preds)/len(preds) # average the predictions\n",
    "    return(avgs)\n",
    "\n",
    "def score(preds,actual):\n",
    "    '''\n",
    "    accepts: preds: an iterable of predictions\n",
    "             actual: the actual values\n",
    "    returns: the R^2 score of the model which generated the predictions\n",
    "    '''\n",
    "    resid = actual - preds.reshape(-1,1)\n",
    "    sse = np.sum(resid**2)\n",
    "    resid_null = actual - np.mean(actual)\n",
    "    sse_null = np.sum(resid_null**2)\n",
    "    r2 = 1-sse/sse_null\n",
    "    return(r2)\n",
    "\n",
    "preds = forest_pred(trees,x_va,fss)\n",
    "score(preds,y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-da9831042483>:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rfr.fit(x_tr,y_tr)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47845143650304056"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn built-in random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(x_tr,y_tr)\n",
    "rfr.score(x_va,y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briggsc1-erau.edu/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quality    0.323588\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our attempt using random subspace projection\n",
    "num_trees = 100\n",
    "trees = []\n",
    "fss = []\n",
    "for i in range(num_trees):\n",
    "    k = np.random.randint(1,13) # of features kept\n",
    "    fs = np.random.uniform(-100,100,k*12).reshape(-1,k)\n",
    "    fs,_ = np.linalg.qr(fs) # gram-schmidt fs (this is a QR-factorization)\n",
    "    np.random.shuffle(fs.T) # in case the columns aren't random, shuffle them.\n",
    "    fss.append(fs)\n",
    "    # here, fs is a 12-by-k orthonormal matrix.\n",
    "    # multiplying (features)*fs projects onto the subspace of R^12 spanned by cols of fs\n",
    "    tr = DecisionTreeRegressor(random_state = np.random.randint(2**32))\n",
    "    tr.fit(np.matmul(x_tr,fs),y_tr)\n",
    "    trees.append(tr)\n",
    "\n",
    "def forest_pred(forest,inputs,fss):\n",
    "    preds = []\n",
    "    for tree,fs in zip(forest,fss):\n",
    "        preds.append(tree.predict(np.matmul(inputs,fs)))\n",
    "    avgs = sum(preds)/len(preds) # average the predictions\n",
    "    return(avgs)\n",
    "\n",
    "preds = forest_pred(trees,x_va,fss)\n",
    "score(preds,y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briggsc1-erau.edu/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quality    0.3427\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could try skewing things a little higher on the number of kept features\n",
    "num_trees = 100\n",
    "trees = []\n",
    "fss = []\n",
    "for i in range(num_trees):\n",
    "    k = max(round(12-np.random.chisquare(3,1).item()),1)\n",
    "    fs = np.random.uniform(-100,100,k*12).reshape(-1,k)\n",
    "    fs,_ = np.linalg.qr(fs) # gram-schmidt X\n",
    "    np.random.shuffle(fs.T) # in case the columns aren't random, shuffle them.\n",
    "    fss.append(fs)\n",
    "    tr = DecisionTreeRegressor(random_state = np.random.randint(2**32))\n",
    "    tr.fit(np.matmul(x_tr,fs),y_tr)\n",
    "    trees.append(tr)\n",
    "\n",
    "def forest_pred(forest,inputs,fss):\n",
    "    preds = []\n",
    "    for tree,fs in zip(forest,fss):\n",
    "        preds.append(tree.predict(np.matmul(inputs,fs)))\n",
    "    avgs = sum(preds)/len(preds) # average the predictions\n",
    "    return(avgs)\n",
    "\n",
    "preds = forest_pred(trees,x_va,fss)\n",
    "score(preds,y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briggsc1-erau.edu/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quality    0.351074\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using normalization, PCA, and random subspace projection\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(normalize(x_tr))\n",
    "x_tr_p = pca.transform(normalize(x_tr))\n",
    "\n",
    "num_trees = 100\n",
    "trees = []\n",
    "fss = []\n",
    "\n",
    "for i in range(num_trees):\n",
    "    k = max(round(12-np.random.chisquare(4,1).item()),1)\n",
    "    fs = np.random.uniform(-100,100,k*12).reshape(-1,k)\n",
    "    fs,_ = np.linalg.qr(fs) # gram-schmidt X\n",
    "    np.random.shuffle(fs.T) # in case the columns aren't random, shuffle them.\n",
    "    fss.append(fs)\n",
    "    tr = DecisionTreeRegressor(random_state = np.random.randint(2**32))\n",
    "    tr.fit(np.matmul(x_tr_p,fs),y_tr)\n",
    "    trees.append(tr)\n",
    "\n",
    "def forest_pred(forest,inputs,fss):\n",
    "    inputs = pca.transform(normalize(inputs))\n",
    "    preds = []\n",
    "    for tree,fs in zip(forest,fss):\n",
    "        preds.append(tree.predict(np.matmul(inputs,fs)))\n",
    "    avgs = sum(preds)/len(preds) # average the predictions\n",
    "    return(avgs)\n",
    "\n",
    "preds = forest_pred(trees,x_va,fss)\n",
    "score(preds,y_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTW your homework assignment is to try normalization, then PCA, then dropping random columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another intervention is to vary the rows involved in training by using bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briggsc1-erau.edu/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3430: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quality    0.465712\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping random features\n",
    "num_trees = 100\n",
    "fss = [] # a list to hold the list of feature subsets\n",
    "for i in range(num_trees):\n",
    "    # we need a random binary array with not all 0s\n",
    "    array = np.random.randint(0,2,12)\n",
    "    while sum(array)==0:\n",
    "        array = np.random.randint(0,2,12)\n",
    "    fs = np.array(features)[[bool(x) for x in np.random.randint(0,2,12)]]\n",
    "    fss.append(df[fs].columns)\n",
    "\n",
    "trees = []\n",
    "for i in range(num_trees):\n",
    "    tr = DecisionTreeRegressor(random_state = np.random.randint(10**7))\n",
    "    bootstrap_index = np.random.choice(x_tr.index,5000)\n",
    "    x_tr_subset = x_tr[fss[i]]\n",
    "    tr.fit(x_tr_subset.loc[bootstrap_index,],y_tr.loc[bootstrap_index,])\n",
    "    trees.append(tr)\n",
    "    \n",
    "def forest_pred(forest,inputs,fss):\n",
    "    preds = []\n",
    "    for tree,fs in zip(forest,fss):\n",
    "        preds.append(tree.predict(inputs[fs]))\n",
    "    avgs = sum(preds)/len(preds) # average the predictions\n",
    "    return(avgs)\n",
    "\n",
    "def score(preds,actual):\n",
    "    '''\n",
    "    accepts: preds: an iterable of predictions\n",
    "             actual: the actual values\n",
    "    returns: the R^2 score of the model which generated the predictions\n",
    "    '''\n",
    "    resid = actual - preds.reshape(-1,1)\n",
    "    sse = np.sum(resid**2)\n",
    "    resid_null = actual - np.mean(actual)\n",
    "    sse_null = np.sum(resid_null**2)\n",
    "    r2 = 1-sse/sse_null\n",
    "    return(r2)\n",
    "\n",
    "preds = forest_pred(trees,x_va,fss)\n",
    "score(preds,y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3967527803842493"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# btw xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(x_tr,y_tr)\n",
    "xgb.score(x_va,y_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/briggsc1-erau.edu/.local/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:570: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41082322171484587"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "xgb = GradientBoostingRegressor()\n",
    "xgb.fit(x_tr,y_tr)\n",
    "xgb.score(x_va,y_va)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
