{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Let's explore CNN components. First we see how convolution layers work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array to which we'll apply the filter\n",
    "data = np.asarray([0, 0, 0, 1, 1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras expects 1D arrays to come in a 3D package. The first dimension is for the sample (i.e. one sample among many), the second dimension is for the length of each array, and the third dimension is for the channel (as in color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(1, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential is a class from keras that lets us stack together layer one-at-a-time into a single model. Conv1D and Conv2D are convolusional layers for application to sets of 1D and 2D arrays respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use one filter with a length of 3, so we'll add one convolution layer to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(1, 3, input_shape=(10, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an edge detector. The final weight is a bias, added to all outputs.\n",
    "weights = [np.asarray([[[0]],[[-1]],[[1]]]), np.asarray([0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the weights in the model\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 0.]],\n",
      "\n",
      "       [[-1.]],\n",
      "\n",
      "       [[ 1.]]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n",
      "[[[ 0.]\n",
      "  [ 1.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [-1.]\n",
      "  [ 0.]\n",
      "  [ 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# apply filter to input data\n",
    "yhat = model.predict(data)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we're really just detecting 1s, and not detecting edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[[ 0.]\n",
      "  [ 0.]\n",
      "  [ 1.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [ 0.]\n",
      "  [-1.]\n",
      "  [ 0.]]]\n"
     ]
    }
   ],
   "source": [
    "weights = [np.asarray([[[-1]],[[1]],[[0]]]), np.asarray([0])]\n",
    "model.set_weights(weights)\n",
    "yhat = model.predict(data)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some edges.\n",
    "\n",
    "Note that the output is smaller than the input. If we wanted to ensure the output had the same size as the input (as in the Mario picture from the video Monday), we could pad the border of our data with 0s before applying the convolution filter.\n",
    "\n",
    "Now we'll apply a filter to a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input data\n",
    "data = np.asarray(\n",
    "    [[[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 1, 0, 0, 0]],\n",
    "     \n",
    "     [[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0]]])\n",
    "\n",
    "data = data.reshape(2, 8, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vertical line detector\n",
    "detector = [[[[-1]],[[1]],[[0]]],\n",
    "            [[[-1]],[[1]],[[0]]],\n",
    "            [[[-1]],[[1]],[[0]]]]\n",
    "weights = [np.asarray(detector), np.asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# apply filter to input data\n",
    "yhat = model.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 2x6x6x1 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 6, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can arrange the output into a 2D grids for nice printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  3.  0. -3.  0.]\n",
      " [ 0.  0.  3.  0. -3.  0.]\n",
      " [ 0.  0.  3.  0. -3.  0.]\n",
      " [ 0.  0.  3.  0. -3.  0.]\n",
      " [ 0.  0.  3.  0. -3.  0.]\n",
      " [ 0.  0.  3.  0. -3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# first input grid, vertical edge detector\n",
    "print(yhat[0,:,:,:].reshape(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# second input grid (horizontal stripe), vertical edge detector\n",
    "print(yhat[1,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that our filter identifies vertical edges but ignores horizontal edges. We can make a filter to detect horizontal edges and ignore vertical edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's include two filters with this model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(2, (3,3), input_shape=(8, 8, 1)))\n",
    "\n",
    "# define vertical and horizontal line detectors\n",
    "detector = [[[[-1,-1]],[[1,-1]],[[0,-1]]],\n",
    "            [[[-1,1]],[[1,1]],[[0,1]]],\n",
    "            [[[-1,0]],[[1,0]],[[0,0]]]]\n",
    "            \n",
    "# our bias vector must have two components\n",
    "weights = [np.asarray(detector), np.asarray([0.0,0])]\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "# apply filter to input data\n",
    "yhat = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6, 6, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  3.,  0., -3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0., -3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0., -3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0., -3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0., -3.,  0.],\n",
       "        [ 0.,  0.,  3.,  0., -3.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the effects of the filters on the first input array\n",
    "yhat[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 3.,  3.,  3.,  3.,  3.,  3.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [-3., -3., -3., -3., -3., -3.]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the effects of the filters on the second input array\n",
    "yhat[:,:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "Pooling is an operation done to a numeric grid (e.g. image), similar to applying a convolution. However, the pooling filter is usually 2x2 and applied with a stride of 2 (no overlap between applications). For each 2x2 subgrid of our image, a scalar is produced. The image is thus down sampled by a factor of 4. Max pooling takes the max of of the four pixels, while average pooling takes the average of the four pixels.\n",
    "\n",
    "The utility of pooling is that sometimes features we're trying to detect may change their location slightly within an image. The down sampling involved in pooling gives us a margin of error for the location of features we're detecting.\n",
    "\n",
    "Let's start with an example using max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we create the data. A 2D array is reshaped to 1 point x 8 rows x 8 columns x 1 channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = [[[[0]],[[-1]],[[1]]],\n",
    "            [[[0]],[[-1]],[[1]]],\n",
    "            [[[0]],[[-1]],[[1]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 1)           10        \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 3, 3, 1)           0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10 (40.00 Byte)\n",
      "Trainable params: 10 (40.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "data = [[0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0]]\n",
    "data = np.asarray(data).reshape(1, 8, 8, 1)\n",
    "\n",
    "# create model and add a convolution layer\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (3,3), activation=None, input_shape=(8, 8, 1)))\n",
    "\n",
    "# add a pooling layer. switch between max and average, and observe the results\n",
    "# model.add(MaxPooling2D())\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2af820be50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[1.5, 0.0, -1.5]\n",
      "[1.5, 0.0, -1.5]\n",
      "[1.5, 0.0, -1.5]\n"
     ]
    }
   ],
   "source": [
    "# define a vertical line detector\n",
    "detector = [[[[0]],[[-1]],[[1]]],\n",
    "            [[[0]],[[-1]],[[1]]],\n",
    "            [[[0]],[[-1]],[[1]]]]\n",
    "weights = [np.asarray(detector), np.asarray([0.0])]\n",
    "# store the weights in the model\n",
    "model.set_weights(weights)\n",
    "# apply filter to input data\n",
    "yhat = model.predict(data)\n",
    "# enumerate rows\n",
    "for r in range(yhat.shape[1]):\n",
    "    # print each column in the row\n",
    "    print([yhat[0,r,c,0] for c in range(yhat.shape[2])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check these results by hand. Can you figure out why the right-most column is still 0s even when we use average pooling? Shouldn't we be taking the average of a negative value and 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Conv2D, Dense, MaxPooling2D, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax')) # output layer\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_tr, y_tr), (x_te, y_te) = mnist.load_data()\n",
    "x_tr = x_tr.reshape((x_tr.shape[0], 28, 28, 1))\n",
    "x_te = x_te.reshape((x_te.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = to_categorical(y_tr) # changes each outcome (integer) into a 1x10 binary array\n",
    "y_te = to_categorical(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "x_tr = x_tr / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te = x_te / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1743 - accuracy: 0.9470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2af816f4f0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 977us/step - loss: 0.0855 - accuracy: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08554726094007492, 0.9732000231742859]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_te,y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bias vector with 32 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 32 filters, each 3x3x1 (grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2480788  -0.12945576  0.04050868]\n",
      " [ 0.36962438  0.56229776 -0.24102975]\n",
      " [-0.37411943  0.66207534 -0.14108403]]\n",
      "[[-0.47312313 -0.07064794 -0.21087766]\n",
      " [-0.6399599  -0.02014964  0.8267993 ]\n",
      " [-0.5691011   0.26392984  0.3229719 ]]\n"
     ]
    }
   ],
   "source": [
    "print(filters[:,:,:,0].reshape(3,3))\n",
    "print(filters[:,:,:,1].reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize them with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(filters[:,:,:,0].reshape(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x864 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes = plt.subplots(8,4,figsize = (6,12))\n",
    "for idx in range(32):\n",
    "    sns.heatmap(filters[:,:,:,idx].reshape(3,3),ax=axes[idx//4,idx%4],cbar = False)\n",
    "plt.setp(axes, xticks=[], yticks=[]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more [here](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
